Record Keeping:

Model: 12
Variables: "sense_temp", "water_temp_log", "open_temp__sum_values", 'sense_hum__quantile__q_0.8'
Rolling: rolling(window = 20).mean()
Lags: 5
Hidden layers: 1 with 64 neurons and 1 with 32
Activation Function: RELU
Learning rate = 0.001
RMSE = 0.21

[60, 70, 75, 80, 80, 90, 90]
[21.764786] [21.936255] [22.13393] 

[50, 50, 50, 50, 50, 60, 57]
[21.733772] [21.85097] [21.995373] 

[35, 35, 35, 36, 36, 37, 40]
[21.709055] [21.785915] [21.869068] 

[30, 31, 32, 33, 35, 35, 35]
[21.703152] [21.772224] [21.845083] 

[34, 34, 35, 35, 35, 35, 36]
[21.708054] [21.783344] [21.863754] 

[30, 30, 30, 31, 32, 32, 32]
[21.70072] [21.765863] [21.833595] 

[30, 29, 28, 26, 27, 25, 25]
[21.697824] [21.758463] [21.819193] 

[30, 27, 26, 25, 23, 22, 22]
[21.69594] [21.752842] [21.808393] 

[20, 20, 21, 22, 21, 20, 19]
[21.692213] [21.744019] [21.792473] 


Model: 11
Variables: "sense_temp", "st_wt", "open_temp__sum_values", 'sense_hum__quantile__q_0.8'
Rolling: rolling(window = 20).mean()
Lags: 5
Hidden layers: 1 with 64 neurons and 1 with 32
Activation Function: RELU
Learning rate = 0.001
RMSE = 0.10

[60, 70, 75, 80, 80, 90, 90]
[21.77357] [21.966558] [22.20211] 

[50, 50, 50, 50, 50, 60, 57]
[21.74708] [21.883806] [22.054888] 

[35, 35, 35, 36, 36, 37, 40]
[21.720036] [21.809212] [21.906208] 

[30, 31, 32, 33, 35, 35, 35]
[21.71269] [21.791813] [21.87478] 

[34, 34, 35, 35, 35, 35, 36]
[21.718748] [21.80575] [21.898588] 

[30, 30, 30, 31, 32, 32, 32]
[21.709723] [21.78401] [21.859812] 

[30, 29, 28, 26, 27, 25, 25]
[21.705576] [21.773424] [21.839891] 

[30, 27, 26, 25, 23, 22, 22]
[21.70246] [21.765099] [21.825186] 

[20, 20, 21, 22, 21, 20, 19]
[21.697395] [21.752861] [21.803223]  

=================================================================================================================================0
Model: 10
Variables: "sense_temp", "water_temp", "open_temp__sum_values", 'sense_hum__quantile__q_0.8'
Rolling: rolling(window = 20).mean()
Lags: 5
Hidden layers: 1 with 64 neurons and 1 with 32
Activation Function: RELU
Learning rate = 0.001
RMSE = 0.15

[60, 70, 75, 80, 80, 90, 90]
[21.784746] [21.944775] [22.073172] 

[50, 50, 50, 50, 50, 60, 57]
[21.746998] [21.869518] [21.997324] 

[35, 35, 35, 36, 36, 37, 40]
[21.714718] [21.79595] [21.881897]

[30, 31, 32, 33, 35, 35, 35]
[21.708353] [21.781685] [21.857882] 

[34, 34, 35, 35, 35, 35, 36]
[21.713501] [21.792883] [21.87607] 

[30, 30, 30, 31, 32, 32, 32]
[21.705763] [21.774834] [21.844976] 

[30, 29, 28, 26, 27, 25, 25]
[21.701904] [21.764572] [21.82566] 

[30, 27, 26, 25, 23, 22, 22]
[21.699467] [21.757494] [21.812931] 

[20, 20, 21, 22, 21, 20, 19]
[21.693586] [21.745195] [21.793629] 